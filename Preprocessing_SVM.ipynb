{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c8b5aeb-a80f-46da-86d4-c6fce46388b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages (2.3.3)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.10.23-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading regex-2025.10.23-cp313-cp313-macosx_11_0_arm64.whl (288 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: pytz, tzdata, tqdm, regex, click, pandas, nltk\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [nltk][32m6/7\u001b[0m [nltk]s]\n",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.3.0 nltk-3.9.2 pandas-2.3.3 pytz-2025.2 regex-2025.10.23 tqdm-4.67.1 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas scikit-learn numpy nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b831478-6693-48cf-a7de-936d2d6580a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nanthansr/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nanthansr/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (33716, 5)\n",
      "Columns: ['Message ID', 'Subject', 'Message', 'Spam/Ham', 'Date']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SVM Preprocessing Pipeline for the Enron Spam Dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "# This notebook cleans, preprocesses, and vectorizes emails so they can be \n",
    "# used with a Support Vector Machine or other classical ML models.\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Reproducibility setup\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Download NLTK resources (only needs to be done once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Load the Enron Spam dataset (CSV format)\n",
    "# Make sure the path matches where you've stored enron_spam_data.csv\n",
    "df = pd.read_csv(\"enron_spam_data.csv\")\n",
    "print(\"Initial shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51c1bac4-2b2e-42ca-8858-0aaf741bec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: (33716, 7)\n",
      "label\n",
      "1    17171\n",
      "0    16545\n",
      "Name: count, dtype: int64\n",
      "After filtering: (33603, 8)\n",
      "Max words now: 1984\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. Combine 'Subject' and 'Message' into a single text field\n",
    "# Some emails may have missing subject or message fields; fillna avoids NaN\n",
    "df[\"text\"] = df[\"Subject\"].fillna('') + \" \" + df[\"Message\"].fillna('')\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5. Encode labels: ham → 0, spam → 1\n",
    "df[\"label\"] = df[\"Spam/Ham\"].map({\"ham\": 0, \"spam\": 1})\n",
    "# Drop any rows that still have missing text or label\n",
    "df = df.dropna(subset=[\"label\", \"text\"])\n",
    "\n",
    "print(\"After cleaning:\", df.shape)\n",
    "print(df[\"label\"].value_counts())  # check class distribution\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 6. Text cleaning function\n",
    "# Lowercases, removes URLs, numbers, punctuation, stopwords, and lemmatizes\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Lowercase, remove punctuation/numbers/stopwords, and lemmatize.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)   # Remove URLs\n",
    "    text = re.sub(r\"\\d+\", \"\", text)              # Remove digits\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply cleaning\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Optionally remove extremely long emails (>2000 words) to reduce noise\n",
    "df = df[df[\"clean_text\"].str.split().apply(len) < 2000]\n",
    "print(\"After filtering:\", df.shape)\n",
    "print(\"Max words now:\", df[\"clean_text\"].str.split().apply(len).max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73891fc7-ada3-4f70-abac-15457929241e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF‑IDF Train Shape: (26882, 5000)\n",
      "TF‑IDF Test Shape: (6721, 5000)\n",
      "\n",
      "Preprocessing complete! Files saved:\n",
      "- X_train_tfidf.npz, X_test_tfidf.npz (vectorized features)\n",
      "- y_train.npy, y_test.npy (label arrays)\n",
      "- tfidf_vectorizer.pkl (fitted vectorizer)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 7. Train–test split\n",
    "# Use stratify=y to preserve class distribution\n",
    "X = df[\"clean_text\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 8. TF‑IDF vectorization\n",
    "# Here we limit features to top 5,000 and include unigrams and bigrams\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)  # don’t re-fit on test data\n",
    "\n",
    "print(\"TF‑IDF Train Shape:\", X_train_tfidf.shape)\n",
    "print(\"TF‑IDF Test Shape:\", X_test_tfidf.shape)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 9. Save processed data for later modeling\n",
    "# Sparse matrices are saved in .npz format to save space\n",
    "save_npz(\"X_train_tfidf.npz\", X_train_tfidf)\n",
    "save_npz(\"X_test_tfidf.npz\", X_test_tfidf)\n",
    "\n",
    "# Labels saved as NumPy arrays\n",
    "np.save(\"y_train.npy\", y_train.to_numpy())\n",
    "np.save(\"y_test.npy\", y_test.to_numpy())\n",
    "\n",
    "# Persist the fitted vectorizer (so you can transform new data later)\n",
    "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "print(\"\\nPreprocessing complete! Files saved:\")\n",
    "print(\"- X_train_tfidf.npz, X_test_tfidf.npz (vectorized features)\")\n",
    "print(\"- y_train.npy, y_test.npy (label arrays)\")\n",
    "print(\"- tfidf_vectorizer.pkl (fitted vectorizer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714c240-d489-4279-b5a1-b564a7c3c8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
